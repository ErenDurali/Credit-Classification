---
title: "Stat 412 Project"
author: "Eren Duralı"
date: "2024-05-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
install.packages("magrittr")
install.packages("tidyverse")
install.packages("mice")
install.packages("zoo")
install.packages("bestNormalize")
install.packages("caret")
install.packages("nnet")
install.packages("stats")
install.packages("MASS")
install.packages("data.table")
install.packages("dplyr")
library(magrittr)
library(tidyverse)
library(mice)
library(zoo)
library(bestNormalize)
library(caret)
library(nnet)
library(stats)
library(MASS)
library(data.table)
library(dplyr)
```



### read the data
```{r}
credit <- read.csv("C:/Users/Eren/Desktop/Ders/Stat412/train.csv")
head(credit,10)
```

```{r}
class(credit)
```
### it is a data frame so we will go with this

```{r}
dim(credit)
```
### we have 100000 observation and 28 variables in the credit data

```{r}
set.seed(412)
str(credit)
```
### age , annual income , num of loan, num of delayed payment, changed credit limit, outstanding debt, amount invested monthly, monthly balance are chr columns but they should be integer so we will change them.

```{r}
library(magrittr)
library(dplyr)
colnames(credit)
credit <- credit %>%
  mutate_at(c(5,8,13,16,17,20,25,27), as.numeric)
str(credit)
```

### we change all columns we want our column names looks good maybe I might think the change all lower cases but it is preference

```{r}
summary(credit)
```
### we have outliers in age and so much missing values also, also num_bank_accounts and num_of_loan, delay from due date, num_of delayed_payment has negative values so we should fix them.

```{r}
library(tidyverse)
colnames(credit) <- str_to_lower(colnames(credit))
credit <- credit %>% 
  filter(abs(age) < 150)
credit <- credit %>%
  filter(abs(num_bank_accounts) < 20) %>%
  mutate(num_bank_accounts = abs(num_bank_accounts))
credit <- credit %>%
  mutate(num_of_loan = abs(num_of_loan)) %>%
  filter(num_of_loan < 25)
credit <- credit %>%
  mutate(delay_from_due_date = abs(delay_from_due_date))
credit <- credit %>%
  mutate(num_of_delayed_payment = abs(num_of_delayed_payment)) %>%
  filter(num_of_delayed_payment < 100)
credit <- credit %>% 
  mutate(changed_credit_limit = abs(changed_credit_limit))
summary(credit)
```
### we fix the issues by throwing the some outliers but we have still so much outliers so we need to find that is there reasonable outputs or not.

```{r}
dim(credit)
```
```{r}
credit_num <- credit[sapply(credit, is.numeric)]
credit_num_sc <- scale(credit_num)
for (i in colnames(credit_num_sc)) {
  boxplot(credit_num_sc[,i], main = i)
  }
```



```{r}
credit_ctg <- credit[sapply(credit, is.character)]
for (i in colnames(credit_ctg)) {
tab <- table(credit_ctg[i])
barplot(tab,main= i,
        ylab="Frequency", col= "red")
  }
```
### I know all of them is not categorical but I want to see that which one I use as a categorical data and how many categories I have.
And Also I see that same values doesn't make any sense so I need to fix that.
### I will delete the data which I don't need such as customer_id, name, ssn (social security number)


```{r}
credit_ctg <- credit_ctg[,-c(1,3,4)]
credit_ctg
which(credit_ctg$payment_of_min_amount == "NM")
which(credit_ctg$payment_behaviour == "!@9#%8")
```

###payment of min amount is a binary operator and it is yes or no and I can't find the explanation for "NM" but it is too much so i can't delete them.
### Also in payment behaviour "!@9#%8" is a unique value too I don't know the meaning of it too but it is not an outlier since it repeats so much

### After this steps I will deal with missing values

```{r}
credit2 <- as.data.frame(bind_cols(credit_num,credit_ctg))
credit2 <- credit2[,-1] # remove id column
```
### I make the new data set with variables I use

```{r}
sum(is.na(credit2))
colSums(is.na(credit2))
```
### We have 34113 missing values :(  and most of missing values in monthly inhand salary so I should check do I need to delete the column or not

```{r}
credit_complete = iris[complete.cases(credit2), ]   #known as complete case
cat("We have", dim(credit2),"dimension before cleaning the missing values", "if we clear all the rows with missing values we will have",dim(credit_complete))
```
### So we need to solve this issue

```{r}
colSums(is.na(credit2))/nrow(credit2)
```
### since no columns have missing values higher than 60% they all stay for now.
### since md.pattern looks like mess I will eleminate the variables for missing part

```{r}
library(mice)
credit_miss <- as.data.frame(credit2[,c(2,3,10,11,12,15,16,21)])
md.pattern(credit_miss)
```
## still It is not clear but we will solve this

```{r}
library(zoo)
credit_num <- as.data.frame(na.approx(credit_num))
sum(is.na(credit_num_sc))

```
### we deal with numerical na's by using na.approx from zoo package this package fill na's by interpolated values lets make a new data and check na's for categorical

```{r}
credit3 <- as.data.frame(bind_cols(credit_num,credit_ctg))
credit3 <- credit3[,-c(1,18,19)]
sum(is.na(credit3))
```

```{r}
colSums(is.na(credit3))
durations <- credit3$credit_history_age
convert_to_months <- function(duration) {
  if (is.na(duration)) {
    return(NA)
  }
  years <- as.numeric(sub("^(\\d+) Years.*$", "\\1", duration))
  months <- as.numeric(sub("^.* and (\\d+) Months$", "\\1", duration))
  total_months <- (years * 12) + months
  return(total_months)
}
total_months <- sapply(durations, convert_to_months)
total_months <- as.numeric(unlist(sapply(durations, convert_to_months)))
credit3$credit_history_age <- total_months
credit3_num <- credit3[sapply(credit3, is.numeric)]
credit3_num <- as.data.frame(na.approx(credit3_num))
credit4 <- as.data.frame(bind_cols(credit3_num,credit_ctg))
credit4 <- credit4[,-c(18,19,22)]
col_name <- colnames(credit4)
col_name[17] <- "credit_history_age"
colnames(credit4) <- col_name
sum(is.na(credit4))
credit4 <- na.omit(credit4)
```

### missing values was in credit_history_age since it is a date I convert that to months so I use as a numeric then fill missing values with na.approx function

### After deal with missing values I want to normalize my data I will do logistic regression and it doesn't have normality assumption but I have so much outliers and it is still have linearity with indep and log odds

```{r}
library(bestNormalize)
credit4_num <- credit4[sapply(credit4, is.numeric)]
credit4_norm_num <- credit4_num
for (i in colnames(credit4_norm_num)) {
  normal <- bestNormalize(credit4_norm_num[[i]], standardize = TRUE)
  credit4_norm_num[[i]] <- normal$x.t
}

```
### lets look at the boxplots

```{r}
for (i in colnames(credit4_norm_num)) {
  boxplot(credit4_norm_num[[i]], main = i)
}
```

### They look better so finally lets make the last data.frame and continue with questions

```{r}
credit4_ctg <- credit4[sapply(credit4, is.character)]
credit5_norm <- as.data.frame(bind_cols(credit4_norm_num,credit4_ctg))
summary(credit5)
```

```{r}
credit5 <- credit5 %>% 
  filter(abs(num_credit_card) < 150)
credit5 <- credit5 %>% 
  filter(abs(num_credit_inquiries) < 150)
```


###Q1) How does annual income influence the number of bank accounts and credit cards an individual holds?
```{r}
q1 <- credit5[,c(2,4,5)]
ggplot(q1, aes(x = annual_income)) + 
  geom_histogram(binwidth = 10000, fill = "blue", color = "black") +
  labs(title = "Distribution of Annual Income", x = "Annual Income", y = "Frequency")

ggplot(q1, aes(x = num_bank_accounts)) + 
  geom_bar(fill = "green", color = "black") +
  labs(title = "Distribution of Number of Bank Accounts", x = "Number of Bank Accounts", y = "Frequency")

ggplot(q1, aes(x = num_credit_card)) + 
  geom_bar(fill = "red", color = "black") +
  labs(title = "Distribution of Number of Credit Cards", x = "Number of Credit Cards", y = "Frequency")

#Scatter plot for relationship visulization
ggplot(q1, aes(x = annual_income, y = num_bank_accounts)) + 
  geom_point() + 
  geom_smooth(method = "lm", col = "blue") + 
  labs(title = "Annual Income vs. Number of Bank Accounts", x = "Annual Income", y = "Number of Bank Accounts")

ggplot(q1, aes(x = annual_income, y = num_credit_card)) + 
  geom_point() + 
  geom_smooth(method = "lm", col = "red") + 
  labs(title = "Annual Income vs. Number of Credit Cards", x = "Annual Income", y = "Number of Credit Cards")
```
### We have two left skewed data maybe we can apply transformation

```{r}
credit5_norm_num <- credit5_norm[sapply(credit5_norm, is.numeric)]
credit5_norm_num_sc <- as.data.frame(scale(credit5_norm_num))
#Calculate correlation coefficients
cor(credit5_norm_num_sc$annual_income, credit5_norm_num_sc$num_bank_accounts)
cor(credit5_norm_num_sc$annual_income, credit5_norm_num_sc$num_credit_card)
## We don't have multicollinearity since it is simple linear regression

# Linear regression
model_bank_accounts <- lm(num_bank_accounts ~ annual_income, data = credit5_norm_num_sc)
summary(model_bank_accounts)

model_credit_cards <- lm(num_credit_card ~ annual_income, data = credit5_norm_num_sc)
summary(model_credit_cards)
```
### annual income looks significant for both number of credit card and number of bank accounts also residuals look good but our R-squared is very low so we could explain the variance of the data in number of credit card nearly 4% and in number of bank account 6%. So annual income has an effect but very little. Annual income has a negative coefficient for both model this means when annual income increase one unit number of credit cards decrease for their coefficient.

#Q2)What is the impact of interest rate and outstanding debt on the credit score of individuals?
```{r}
ggplot(credit5_norm, aes(x = interest_rate)) + 
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black") +
  labs(title = "Distribution of Interest Rate", x = "Interest Rate", y = "Frequency")

ggplot(credit5_norm, aes(x = outstanding_debt)) + 
  geom_histogram(binwidth = 5000, fill = "green", color = "black") +
  labs(title = "Distribution of Outstanding Debt", x = "Outstanding Debt", y = "Frequency")

ggplot(credit5_norm, aes(x = credit_score)) + 
  geom_bar(fill = "red", color = "black") +
  labs(title = "Distribution of Credit Score", x = "Credit Score", y = "Frequency")

ggplot(credit5_norm, aes(x = interest_rate, y = credit_score)) + 
  geom_point() + 
  geom_smooth(method = "lm", col = "blue") + 
  labs(title = "Interest Rate vs. Credit Score", x = "Interest Rate", y = "Credit Score")

ggplot(credit5_norm, aes(x = outstanding_debt, y = credit_score)) + 
  geom_point() + 
  geom_smooth(method = "lm", col = "green") + 
  labs(title = "Outstanding Debt vs. Credit Score", x = "Outstanding Debt", y = "Credit Score")
```
```{r}
### Encode credit_score with label encoding
library(caret)
library(nnet)
credit5_norm$credit_score <- factor(credit5_norm$credit_score, levels = c('Poor', 'Standard', 'Good'), labels = c(1, 2, 3))
credit5_norm$credit_score <- as.numeric(as.character(credit5_norm$credit_score))
```


```{r}
library(nnet)
model_credit_score <- multinom(credit_score ~ interest_rate + outstanding_debt, data = credit5_norm)
summary(model_credit_score)
coefficients <- summary(model_credit_score)$coefficients
print(coefficients)
# Calculate odds ratios
odds_ratios <- exp(coefficients)
print(odds_ratios)
# Calculate confidence intervals for the coefficients
standard_errors <- summary(model_credit_score)$standard.errors
z <- coefficients / standard_errors
p_values <- (1 - pnorm(abs(z), 0, 1)) * 2
# Confidence intervals
conf_int_lower <- exp(coefficients - 1.96 * standard_errors)
conf_int_upper <- exp(coefficients + 1.96 * standard_errors)
# Display odds ratios with confidence intervals
odds_ratios_ci <- data.frame(
  OR = odds_ratios,
  CI_lower = conf_int_lower,
  CI_upper = conf_int_upper,
  p_values = p_values
)
print(odds_ratios_ci)
```
## Intercept represent "Poor".First look coefficents For a one-unit increase in the interest rate, the log-odds of having a "Standard" credit score (versus "Poor") decrease by 0.3621638.For a one-unit increase in outstanding debt, the log-odds of having a "Standard" credit score (versus "Poor") decrease by 0.6468804.Similarly in the Good vs Poor we have a negative coef.For a one-unit increase in the interest rate, the log-odds of having a "Good" credit score (versus "Poor") decrease by 1.1864338.For a one-unit increase in outstanding debt, the log-odds of having a "Good" credit score (versus "Poor") decrease by 0.8583294.When we get the odds ratios. For a one-unit increase in the interest rate, the odds of having a "Standard" credit score (versus "Poor") are multiplied by approximately 0.696 (a decrease in odds).For a one-unit increase in outstanding debt, the odds of having a "Standard" credit score (versus "Poor") are multiplied by approximately 0.524 (a decrease in odds). For a one-unit increase in the interest rate, the odds of having a "Good" credit score (versus "Poor") are multiplied by approximately 0.305 (a decrease in odds).For a one-unit increase in outstanding debt, the odds of having a "Good" credit score (versus "Poor") are multiplied by approximately 0.424 (a decrease in odds). 
## Both interest_rate and outstanding_debt have a negative impact on the likelihood of having a "Standard" or "Good" credit score compared to a "Poor" credit score. This is evidenced by the odds ratios being less than 1 for both predictors in both comparisons, indicating a decrease in the odds of having a better credit score with higher interest rates and outstanding debts.
## Both of them are significant since their p-values < 0.05 and since CI's not containing 1 one unit increase has an effect. For every one unit increase in interest rate; The odds of having a "Standard" credit score versus a "Poor" credit score decrease by approximately 30.38%.The odds of having a "Good" credit score versus a "Poor" credit score decrease by approximately 69.47%.For every one-unit increase in outstanding debt; The odds of having a "Standard" credit score versus a "Poor" credit score decrease by approximately 47.63%.The odds of having a "Good" credit score versus a "Poor" credit score decrease by approximately 57.61%.

#Q3)Is there a significant association between the type of loan (e.g., mortgage, car loan, personal loan) and the frequency of late payments?

```{r}
contingency_table <- table(credit5_norm$type_of_loan, credit5_norm$num_of_delayed_payment)

# Chi-square test
chi_sq_test <- chisq.test(contingency_table)
print(chi_sq_test)

# Calculate Cramér's V 
cramers_v <- sqrt(chi_sq_test$statistic / sum(contingency_table) * (min(nrow(contingency_table), ncol(contingency_table)) - 1))
print(cramers_v)
```
Since Cramer's V = 17.11 type of loan and the frequency of delayed payments has a strong association. This shows that type of loan has a significant impact on the frequency of delayed payments.

Q4) Does the age of individuals vary significantly across different levels of payment behavior?
```{r}
anova_result <- aov(age ~ payment_behaviour, data = credit5_norm)
summary(anova_result)

# Perform post-hoc tests if ANOVA is significant
posthoc <- TukeyHSD(anova_result)
print(posthoc)
```

Since P score <2e-16 lower than 0.05 payment behaviour is associated with variations on age.Individuals with "High_spent_Large_value_payments" tend to be older compared to those with "High_spent_Medium_value_payments" and "High_spent_Small_value_payments" (p < 0.05).There are significant age differences between various combinations of payment behavior categories, such as "Low_spent_Small_value_payments" versus "High_spent_Small_value_payments" (p < 0.05).

Q5)

```{r}
# credit mix encoding
credit5_norm$credit_mix[credit5_norm$credit_mix == "_"] <- "undefined"
credit5_norm$credit_mix <- factor(credit5_norm$credit_mix, levels = c("undefined",'Bad', 'Standard', 'Good'), labels = c(0, 1, 2, 3))
credit5_norm$credit_mix <- as.numeric(as.character(credit5_norm$credit_mix))
# type of loan encoding
credit5_norm$type_of_loan[credit5_norm$type_of_loan == ""] <- "undefined"
count_encoding <- data.table(credit5_norm)[, .(count = .N), by = type_of_loan]
credit5_norm <- merge(credit5_norm, count_encoding, by = "type_of_loan", all.x = TRUE)
setnames(credit5_norm, "count", "type_of_loan_encoded")
#payment_behaviour encoding
count_encoding <- data.table(credit5_norm)[, .(count = .N), by = payment_behaviour]
credit5_norm <- merge(credit5_norm, count_encoding, by = "payment_behaviour", all.x = TRUE)
setnames(credit5_norm, "count", "payment_behaviour_encoded")
colSums(is.na(credit5_norm))
credit5_norm <- credit5_norm[,-c(1,2)]
```




```{r}
set.seed(412)
credit5_norm$credit_score <- as.factor(credit5_norm$credit_score)
credit5_norm$credit_mix <- as.factor(credit5_norm$credit_mix)
credit5_norm$type_of_loan_encoded <- as.factor(credit5_norm$type_of_loan_encoded)
credit5_norm$payment_behaviour_encoded <- as.factor(credit5_norm$payment_behaviour_encoded)
# Split data
train_index <- createDataPartition(credit5_norm$credit_score, p = 0.8, list = FALSE)
train_data <- credit5_norm[train_index, ]
test_data <- credit5_norm[-train_index, ]
# cross-validation
cv_control <- trainControl(method = "cv", number = 5)
# Train multinomial logistic regression model with PCA
multinom_pca_model <- train(
  credit_score ~ ., 
  data = train_data, 
  method = "multinom", 
  trControl = cv_control,
  preProcess = c("center", "scale", "pca"),
  tuneLength = 10
)
# Predict credit scores on test set
predictions_pca <- predict(multinom_pca_model, newdata = test_data)

# Model performance
confusionMatrix(predictions_pca, test_data$credit_score)
```
### The confusion matrix shows the counts of correct and incorrect predictions for each class. It looks good but it can be better also Models accuracy is 61.24%. Model accuracy achieved by always predicting the most frequent class around 53.26%. A measure of agreement between actual and predicted classes, adjusted for chance agreement. Here, it's around 0.3343, indicating fair agreement. Sensitivity is higher for class 2 72.9% but sensitivity in the other class below 50% so we should be deal with that.

```{r}
library(corrplot)
corr = cor(credit5_norm_num)
corrplot(corr, method = "number", type = "upper", diag = FALSE,number.cex = 0.7)
```


